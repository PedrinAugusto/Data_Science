{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes Neurais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# caminho da pasta onde está o arquivo \n",
    "caminho = 'C:/Users/User/OneDrive/Capacitação/Portifolio_GitHub/Data_Science/data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base de Crédito"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(caminho + 'credit.pkl', 'rb') as f:\n",
    "    X_credit_treinamento, y_credit_treinamento, X_credit_teste, y_credit_teste = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.03075466\n",
      "Iteration 2, loss = 0.93903681\n",
      "Iteration 3, loss = 0.85555604\n",
      "Iteration 4, loss = 0.77808581\n",
      "Iteration 5, loss = 0.70736625\n",
      "Iteration 6, loss = 0.64083093\n",
      "Iteration 7, loss = 0.58035554\n",
      "Iteration 8, loss = 0.52437146\n",
      "Iteration 9, loss = 0.47414326\n",
      "Iteration 10, loss = 0.42943131\n",
      "Iteration 11, loss = 0.39057514\n",
      "Iteration 12, loss = 0.35806834\n",
      "Iteration 13, loss = 0.32991127\n",
      "Iteration 14, loss = 0.30552111\n",
      "Iteration 15, loss = 0.28392283\n",
      "Iteration 16, loss = 0.26441541\n",
      "Iteration 17, loss = 0.24688904\n",
      "Iteration 18, loss = 0.23097426\n",
      "Iteration 19, loss = 0.21692673\n",
      "Iteration 20, loss = 0.20452487\n",
      "Iteration 21, loss = 0.19342868\n",
      "Iteration 22, loss = 0.18365680\n",
      "Iteration 23, loss = 0.17483199\n",
      "Iteration 24, loss = 0.16674297\n",
      "Iteration 25, loss = 0.15947660\n",
      "Iteration 26, loss = 0.15284338\n",
      "Iteration 27, loss = 0.14667248\n",
      "Iteration 28, loss = 0.14102094\n",
      "Iteration 29, loss = 0.13580316\n",
      "Iteration 30, loss = 0.13090700\n",
      "Iteration 31, loss = 0.12645605\n",
      "Iteration 32, loss = 0.12219534\n",
      "Iteration 33, loss = 0.11839390\n",
      "Iteration 34, loss = 0.11477010\n",
      "Iteration 35, loss = 0.11136653\n",
      "Iteration 36, loss = 0.10826935\n",
      "Iteration 37, loss = 0.10543304\n",
      "Iteration 38, loss = 0.10273219\n",
      "Iteration 39, loss = 0.10012836\n",
      "Iteration 40, loss = 0.09776232\n",
      "Iteration 41, loss = 0.09551304\n",
      "Iteration 42, loss = 0.09335517\n",
      "Iteration 43, loss = 0.09132369\n",
      "Iteration 44, loss = 0.08941125\n",
      "Iteration 45, loss = 0.08765742\n",
      "Iteration 46, loss = 0.08600149\n",
      "Iteration 47, loss = 0.08430980\n",
      "Iteration 48, loss = 0.08277361\n",
      "Iteration 49, loss = 0.08132559\n",
      "Iteration 50, loss = 0.08000880\n",
      "Iteration 51, loss = 0.07854677\n",
      "Iteration 52, loss = 0.07734820\n",
      "Iteration 53, loss = 0.07609424\n",
      "Iteration 54, loss = 0.07476661\n",
      "Iteration 55, loss = 0.07356959\n",
      "Iteration 56, loss = 0.07252164\n",
      "Iteration 57, loss = 0.07134620\n",
      "Iteration 58, loss = 0.07020277\n",
      "Iteration 59, loss = 0.06931764\n",
      "Iteration 60, loss = 0.06813495\n",
      "Iteration 61, loss = 0.06722235\n",
      "Iteration 62, loss = 0.06615158\n",
      "Iteration 63, loss = 0.06520164\n",
      "Iteration 64, loss = 0.06427750\n",
      "Iteration 65, loss = 0.06333774\n",
      "Iteration 66, loss = 0.06246364\n",
      "Iteration 67, loss = 0.06150637\n",
      "Iteration 68, loss = 0.06058167\n",
      "Iteration 69, loss = 0.05975804\n",
      "Iteration 70, loss = 0.05888977\n",
      "Iteration 71, loss = 0.05804621\n",
      "Iteration 72, loss = 0.05736933\n",
      "Iteration 73, loss = 0.05654274\n",
      "Iteration 74, loss = 0.05566839\n",
      "Iteration 75, loss = 0.05490683\n",
      "Iteration 76, loss = 0.05414713\n",
      "Iteration 77, loss = 0.05322972\n",
      "Iteration 78, loss = 0.05254739\n",
      "Iteration 79, loss = 0.05172390\n",
      "Iteration 80, loss = 0.05097214\n",
      "Iteration 81, loss = 0.05021040\n",
      "Iteration 82, loss = 0.04955555\n",
      "Iteration 83, loss = 0.04879740\n",
      "Iteration 84, loss = 0.04794730\n",
      "Iteration 85, loss = 0.04734975\n",
      "Iteration 86, loss = 0.04650216\n",
      "Iteration 87, loss = 0.04610281\n",
      "Iteration 88, loss = 0.04528130\n",
      "Iteration 89, loss = 0.04457522\n",
      "Iteration 90, loss = 0.04400081\n",
      "Iteration 91, loss = 0.04338196\n",
      "Iteration 92, loss = 0.04277628\n",
      "Iteration 93, loss = 0.04221233\n",
      "Iteration 94, loss = 0.04173456\n",
      "Iteration 95, loss = 0.04110761\n",
      "Iteration 96, loss = 0.04071690\n",
      "Iteration 97, loss = 0.03994849\n",
      "Iteration 98, loss = 0.03935744\n",
      "Iteration 99, loss = 0.03880103\n",
      "Iteration 100, loss = 0.03820770\n",
      "Iteration 101, loss = 0.03771397\n",
      "Iteration 102, loss = 0.03709518\n",
      "Iteration 103, loss = 0.03655083\n",
      "Iteration 104, loss = 0.03598168\n",
      "Iteration 105, loss = 0.03546151\n",
      "Iteration 106, loss = 0.03496378\n",
      "Iteration 107, loss = 0.03449852\n",
      "Iteration 108, loss = 0.03401178\n",
      "Iteration 109, loss = 0.03359122\n",
      "Iteration 110, loss = 0.03326994\n",
      "Iteration 111, loss = 0.03280644\n",
      "Iteration 112, loss = 0.03236299\n",
      "Iteration 113, loss = 0.03212644\n",
      "Iteration 114, loss = 0.03182810\n",
      "Iteration 115, loss = 0.03130291\n",
      "Iteration 116, loss = 0.03098309\n",
      "Iteration 117, loss = 0.03080306\n",
      "Iteration 118, loss = 0.03044682\n",
      "Iteration 119, loss = 0.03002277\n",
      "Iteration 120, loss = 0.02979927\n",
      "Iteration 121, loss = 0.02945795\n",
      "Iteration 122, loss = 0.02922717\n",
      "Iteration 123, loss = 0.02896001\n",
      "Iteration 124, loss = 0.02870662\n",
      "Iteration 125, loss = 0.02835933\n",
      "Iteration 126, loss = 0.02805128\n",
      "Iteration 127, loss = 0.02774663\n",
      "Iteration 128, loss = 0.02751906\n",
      "Iteration 129, loss = 0.02724841\n",
      "Iteration 130, loss = 0.02701795\n",
      "Iteration 131, loss = 0.02684392\n",
      "Iteration 132, loss = 0.02657940\n",
      "Iteration 133, loss = 0.02639044\n",
      "Iteration 134, loss = 0.02601566\n",
      "Iteration 135, loss = 0.02581922\n",
      "Iteration 136, loss = 0.02568150\n",
      "Iteration 137, loss = 0.02540786\n",
      "Iteration 138, loss = 0.02513270\n",
      "Iteration 139, loss = 0.02513772\n",
      "Iteration 140, loss = 0.02469725\n",
      "Iteration 141, loss = 0.02456593\n",
      "Iteration 142, loss = 0.02429634\n",
      "Iteration 143, loss = 0.02417911\n",
      "Iteration 144, loss = 0.02397755\n",
      "Iteration 145, loss = 0.02369811\n",
      "Iteration 146, loss = 0.02354343\n",
      "Iteration 147, loss = 0.02344353\n",
      "Iteration 148, loss = 0.02316843\n",
      "Iteration 149, loss = 0.02300791\n",
      "Iteration 150, loss = 0.02280341\n",
      "Iteration 151, loss = 0.02263999\n",
      "Iteration 152, loss = 0.02254357\n",
      "Iteration 153, loss = 0.02226959\n",
      "Iteration 154, loss = 0.02219673\n",
      "Iteration 155, loss = 0.02193881\n",
      "Iteration 156, loss = 0.02181863\n",
      "Iteration 157, loss = 0.02165557\n",
      "Iteration 158, loss = 0.02153772\n",
      "Iteration 159, loss = 0.02137257\n",
      "Iteration 160, loss = 0.02118812\n",
      "Iteration 161, loss = 0.02128531\n",
      "Iteration 162, loss = 0.02093056\n",
      "Iteration 163, loss = 0.02069334\n",
      "Iteration 164, loss = 0.02064798\n",
      "Iteration 165, loss = 0.02049150\n",
      "Iteration 166, loss = 0.02031834\n",
      "Iteration 167, loss = 0.02013739\n",
      "Iteration 168, loss = 0.02001944\n",
      "Iteration 169, loss = 0.01990047\n",
      "Iteration 170, loss = 0.01971528\n",
      "Iteration 171, loss = 0.01966526\n",
      "Iteration 172, loss = 0.01948102\n",
      "Iteration 173, loss = 0.01934802\n",
      "Iteration 174, loss = 0.01922624\n",
      "Iteration 175, loss = 0.01906894\n",
      "Iteration 176, loss = 0.01901792\n",
      "Iteration 177, loss = 0.01879575\n",
      "Iteration 178, loss = 0.01889993\n",
      "Iteration 179, loss = 0.01855966\n",
      "Iteration 180, loss = 0.01851197\n",
      "Iteration 181, loss = 0.01840169\n",
      "Iteration 182, loss = 0.01821772\n",
      "Iteration 183, loss = 0.01807952\n",
      "Iteration 184, loss = 0.01794865\n",
      "Iteration 185, loss = 0.01783035\n",
      "Iteration 186, loss = 0.01776259\n",
      "Iteration 187, loss = 0.01760217\n",
      "Iteration 188, loss = 0.01755449\n",
      "Iteration 189, loss = 0.01745221\n",
      "Iteration 190, loss = 0.01727856\n",
      "Iteration 191, loss = 0.01715621\n",
      "Iteration 192, loss = 0.01710988\n",
      "Iteration 193, loss = 0.01713940\n",
      "Iteration 194, loss = 0.01684684\n",
      "Iteration 195, loss = 0.01670392\n",
      "Iteration 196, loss = 0.01667311\n",
      "Iteration 197, loss = 0.01650405\n",
      "Iteration 198, loss = 0.01641547\n",
      "Iteration 199, loss = 0.01632475\n",
      "Iteration 200, loss = 0.01625650\n",
      "Iteration 201, loss = 0.01620547\n",
      "Iteration 202, loss = 0.01603408\n",
      "Iteration 203, loss = 0.01601553\n",
      "Iteration 204, loss = 0.01585419\n",
      "Iteration 205, loss = 0.01577962\n",
      "Iteration 206, loss = 0.01570583\n",
      "Iteration 207, loss = 0.01557190\n",
      "Iteration 208, loss = 0.01553297\n",
      "Iteration 209, loss = 0.01532300\n",
      "Iteration 210, loss = 0.01534834\n",
      "Iteration 211, loss = 0.01534517\n",
      "Iteration 212, loss = 0.01507435\n",
      "Iteration 213, loss = 0.01512220\n",
      "Iteration 214, loss = 0.01491210\n",
      "Iteration 215, loss = 0.01496477\n",
      "Iteration 216, loss = 0.01486148\n",
      "Iteration 217, loss = 0.01465201\n",
      "Iteration 218, loss = 0.01461847\n",
      "Iteration 219, loss = 0.01456016\n",
      "Iteration 220, loss = 0.01447653\n",
      "Iteration 221, loss = 0.01444089\n",
      "Iteration 222, loss = 0.01429736\n",
      "Iteration 223, loss = 0.01417462\n",
      "Iteration 224, loss = 0.01412121\n",
      "Iteration 225, loss = 0.01402537\n",
      "Iteration 226, loss = 0.01398175\n",
      "Iteration 227, loss = 0.01395480\n",
      "Iteration 228, loss = 0.01398962\n",
      "Iteration 229, loss = 0.01385671\n",
      "Iteration 230, loss = 0.01362882\n",
      "Iteration 231, loss = 0.01358233\n",
      "Iteration 232, loss = 0.01353830\n",
      "Iteration 233, loss = 0.01338871\n",
      "Iteration 234, loss = 0.01330454\n",
      "Iteration 235, loss = 0.01326448\n",
      "Iteration 236, loss = 0.01320340\n",
      "Iteration 237, loss = 0.01318913\n",
      "Iteration 238, loss = 0.01303135\n",
      "Iteration 239, loss = 0.01302702\n",
      "Iteration 240, loss = 0.01294562\n",
      "Iteration 241, loss = 0.01293189\n",
      "Iteration 242, loss = 0.01284499\n",
      "Iteration 243, loss = 0.01272797\n",
      "Iteration 244, loss = 0.01259231\n",
      "Iteration 245, loss = 0.01263496\n",
      "Iteration 246, loss = 0.01247261\n",
      "Iteration 247, loss = 0.01243261\n",
      "Iteration 248, loss = 0.01247494\n",
      "Iteration 249, loss = 0.01253412\n",
      "Iteration 250, loss = 0.01223455\n",
      "Iteration 251, loss = 0.01220915\n",
      "Iteration 252, loss = 0.01218808\n",
      "Iteration 253, loss = 0.01210828\n",
      "Iteration 254, loss = 0.01197585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 255, loss = 0.01197508\n",
      "Iteration 256, loss = 0.01211306\n",
      "Iteration 257, loss = 0.01206885\n",
      "Iteration 258, loss = 0.01174800\n",
      "Iteration 259, loss = 0.01172034\n",
      "Iteration 260, loss = 0.01168396\n",
      "Iteration 261, loss = 0.01162162\n",
      "Iteration 262, loss = 0.01160717\n",
      "Iteration 263, loss = 0.01148756\n",
      "Iteration 264, loss = 0.01144263\n",
      "Iteration 265, loss = 0.01142419\n",
      "Iteration 266, loss = 0.01135268\n",
      "Iteration 267, loss = 0.01137784\n",
      "Iteration 268, loss = 0.01121572\n",
      "Iteration 269, loss = 0.01105850\n",
      "Iteration 270, loss = 0.01112507\n",
      "Iteration 271, loss = 0.01099137\n",
      "Iteration 272, loss = 0.01101338\n",
      "Iteration 273, loss = 0.01097213\n",
      "Iteration 274, loss = 0.01092945\n",
      "Iteration 275, loss = 0.01082860\n",
      "Iteration 276, loss = 0.01078130\n",
      "Iteration 277, loss = 0.01073247\n",
      "Iteration 278, loss = 0.01064372\n",
      "Iteration 279, loss = 0.01056125\n",
      "Iteration 280, loss = 0.01054657\n",
      "Iteration 281, loss = 0.01051678\n",
      "Iteration 282, loss = 0.01042910\n",
      "Iteration 283, loss = 0.01044659\n",
      "Iteration 284, loss = 0.01035173\n",
      "Iteration 285, loss = 0.01023241\n",
      "Iteration 286, loss = 0.01030120\n",
      "Iteration 287, loss = 0.01014254\n",
      "Iteration 288, loss = 0.01021807\n",
      "Iteration 289, loss = 0.01020836\n",
      "Iteration 290, loss = 0.01002647\n",
      "Iteration 291, loss = 0.01005148\n",
      "Iteration 292, loss = 0.01001091\n",
      "Iteration 293, loss = 0.00988886\n",
      "Iteration 294, loss = 0.00993772\n",
      "Iteration 295, loss = 0.00985990\n",
      "Iteration 296, loss = 0.00981571\n",
      "Iteration 297, loss = 0.00967258\n",
      "Iteration 298, loss = 0.00967599\n",
      "Iteration 299, loss = 0.00968625\n",
      "Iteration 300, loss = 0.00959561\n",
      "Iteration 301, loss = 0.00962205\n",
      "Iteration 302, loss = 0.00943015\n",
      "Iteration 303, loss = 0.00973456\n",
      "Iteration 304, loss = 0.00948057\n",
      "Iteration 305, loss = 0.00937225\n",
      "Iteration 306, loss = 0.00931580\n",
      "Iteration 307, loss = 0.00937002\n",
      "Iteration 308, loss = 0.00919656\n",
      "Iteration 309, loss = 0.00912831\n",
      "Iteration 310, loss = 0.00913565\n",
      "Iteration 311, loss = 0.00904420\n",
      "Iteration 312, loss = 0.00913098\n",
      "Iteration 313, loss = 0.00902821\n",
      "Iteration 314, loss = 0.00901047\n",
      "Iteration 315, loss = 0.00892477\n",
      "Iteration 316, loss = 0.00893184\n",
      "Iteration 317, loss = 0.00880403\n",
      "Iteration 318, loss = 0.00882965\n",
      "Iteration 319, loss = 0.00879578\n",
      "Iteration 320, loss = 0.00868706\n",
      "Iteration 321, loss = 0.00878054\n",
      "Iteration 322, loss = 0.00859559\n",
      "Iteration 323, loss = 0.00861234\n",
      "Iteration 324, loss = 0.00878432\n",
      "Iteration 325, loss = 0.00870712\n",
      "Iteration 326, loss = 0.00857828\n",
      "Iteration 327, loss = 0.00841103\n",
      "Iteration 328, loss = 0.00839490\n",
      "Iteration 329, loss = 0.00845082\n",
      "Iteration 330, loss = 0.00832651\n",
      "Iteration 331, loss = 0.00836285\n",
      "Iteration 332, loss = 0.00836509\n",
      "Iteration 333, loss = 0.00824888\n",
      "Iteration 334, loss = 0.00825763\n",
      "Iteration 335, loss = 0.00816033\n",
      "Iteration 336, loss = 0.00814220\n",
      "Iteration 337, loss = 0.00812539\n",
      "Iteration 338, loss = 0.00807210\n",
      "Iteration 339, loss = 0.00801466\n",
      "Iteration 340, loss = 0.00816280\n",
      "Iteration 341, loss = 0.00791429\n",
      "Iteration 342, loss = 0.00794025\n",
      "Iteration 343, loss = 0.00792933\n",
      "Iteration 344, loss = 0.00788831\n",
      "Iteration 345, loss = 0.00778191\n",
      "Iteration 346, loss = 0.00773740\n",
      "Iteration 347, loss = 0.00769169\n",
      "Iteration 348, loss = 0.00766429\n",
      "Iteration 349, loss = 0.00774452\n",
      "Iteration 350, loss = 0.00762288\n",
      "Iteration 351, loss = 0.00770070\n",
      "Iteration 352, loss = 0.00750372\n",
      "Iteration 353, loss = 0.00775674\n",
      "Iteration 354, loss = 0.00752040\n",
      "Iteration 355, loss = 0.00741114\n",
      "Iteration 356, loss = 0.00756012\n",
      "Iteration 357, loss = 0.00743027\n",
      "Iteration 358, loss = 0.00738240\n",
      "Iteration 359, loss = 0.00747837\n",
      "Iteration 360, loss = 0.00732924\n",
      "Iteration 361, loss = 0.00730222\n",
      "Iteration 362, loss = 0.00734038\n",
      "Iteration 363, loss = 0.00721137\n",
      "Iteration 364, loss = 0.00720302\n",
      "Iteration 365, loss = 0.00728292\n",
      "Iteration 366, loss = 0.00712568\n",
      "Iteration 367, loss = 0.00704293\n",
      "Iteration 368, loss = 0.00707460\n",
      "Iteration 369, loss = 0.00697661\n",
      "Iteration 370, loss = 0.00696723\n",
      "Iteration 371, loss = 0.00696474\n",
      "Iteration 372, loss = 0.00689918\n",
      "Iteration 373, loss = 0.00689187\n",
      "Iteration 374, loss = 0.00689951\n",
      "Iteration 375, loss = 0.00681796\n",
      "Iteration 376, loss = 0.00680701\n",
      "Iteration 377, loss = 0.00680366\n",
      "Iteration 378, loss = 0.00673346\n",
      "Iteration 379, loss = 0.00677451\n",
      "Iteration 380, loss = 0.00670649\n",
      "Iteration 381, loss = 0.00658034\n",
      "Iteration 382, loss = 0.00669512\n",
      "Iteration 383, loss = 0.00651770\n",
      "Iteration 384, loss = 0.00671739\n",
      "Iteration 385, loss = 0.00657982\n",
      "Iteration 386, loss = 0.00661664\n",
      "Iteration 387, loss = 0.00648085\n",
      "Iteration 388, loss = 0.00650215\n",
      "Iteration 389, loss = 0.00639775\n",
      "Iteration 390, loss = 0.00651395\n",
      "Iteration 391, loss = 0.00635775\n",
      "Iteration 392, loss = 0.00655794\n",
      "Iteration 393, loss = 0.00643567\n",
      "Iteration 394, loss = 0.00629318\n",
      "Iteration 395, loss = 0.00631733\n",
      "Iteration 396, loss = 0.00627579\n",
      "Iteration 397, loss = 0.00627389\n",
      "Iteration 398, loss = 0.00618353\n",
      "Iteration 399, loss = 0.00617091\n",
      "Iteration 400, loss = 0.00610174\n",
      "Iteration 401, loss = 0.00609390\n",
      "Iteration 402, loss = 0.00626613\n",
      "Iteration 403, loss = 0.00611543\n",
      "Iteration 404, loss = 0.00600869\n",
      "Iteration 405, loss = 0.00605048\n",
      "Iteration 406, loss = 0.00609058\n",
      "Iteration 407, loss = 0.00600890\n",
      "Iteration 408, loss = 0.00600796\n",
      "Iteration 409, loss = 0.00592225\n",
      "Iteration 410, loss = 0.00587543\n",
      "Iteration 411, loss = 0.00589224\n",
      "Iteration 412, loss = 0.00584272\n",
      "Iteration 413, loss = 0.00586680\n",
      "Iteration 414, loss = 0.00591622\n",
      "Iteration 415, loss = 0.00589106\n",
      "Iteration 416, loss = 0.00579859\n",
      "Iteration 417, loss = 0.00569136\n",
      "Iteration 418, loss = 0.00589791\n",
      "Iteration 419, loss = 0.00560643\n",
      "Iteration 420, loss = 0.00571434\n",
      "Iteration 421, loss = 0.00588996\n",
      "Iteration 422, loss = 0.00565027\n",
      "Iteration 423, loss = 0.00565605\n",
      "Iteration 424, loss = 0.00554901\n",
      "Iteration 425, loss = 0.00555164\n",
      "Iteration 426, loss = 0.00550830\n",
      "Iteration 427, loss = 0.00557869\n",
      "Iteration 428, loss = 0.00545606\n",
      "Iteration 429, loss = 0.00546701\n",
      "Iteration 430, loss = 0.00544374\n",
      "Iteration 431, loss = 0.00538327\n",
      "Iteration 432, loss = 0.00545299\n",
      "Iteration 433, loss = 0.00549119\n",
      "Iteration 434, loss = 0.00527569\n",
      "Iteration 435, loss = 0.00547545\n",
      "Iteration 436, loss = 0.00527637\n",
      "Iteration 437, loss = 0.00527536\n",
      "Iteration 438, loss = 0.00527433\n",
      "Iteration 439, loss = 0.00524474\n",
      "Iteration 440, loss = 0.00524091\n",
      "Iteration 441, loss = 0.00511578\n",
      "Iteration 442, loss = 0.00519078\n",
      "Iteration 443, loss = 0.00513314\n",
      "Iteration 444, loss = 0.00508591\n",
      "Iteration 445, loss = 0.00511148\n",
      "Iteration 446, loss = 0.00507534\n",
      "Iteration 447, loss = 0.00510407\n",
      "Iteration 448, loss = 0.00504313\n",
      "Iteration 449, loss = 0.00498562\n",
      "Iteration 450, loss = 0.00507947\n",
      "Iteration 451, loss = 0.00510491\n",
      "Iteration 452, loss = 0.00493481\n",
      "Iteration 453, loss = 0.00495066\n",
      "Iteration 454, loss = 0.00495071\n",
      "Iteration 455, loss = 0.00483446\n",
      "Iteration 456, loss = 0.00489700\n",
      "Iteration 457, loss = 0.00496547\n",
      "Iteration 458, loss = 0.00480657\n",
      "Iteration 459, loss = 0.00494459\n",
      "Iteration 460, loss = 0.00482463\n",
      "Iteration 461, loss = 0.00477232\n",
      "Iteration 462, loss = 0.00476244\n",
      "Iteration 463, loss = 0.00474472\n",
      "Iteration 464, loss = 0.00471482\n",
      "Iteration 465, loss = 0.00460965\n",
      "Iteration 466, loss = 0.00489651\n",
      "Iteration 467, loss = 0.00465801\n",
      "Iteration 468, loss = 0.00465923\n",
      "Iteration 469, loss = 0.00463767\n",
      "Iteration 470, loss = 0.00460318\n",
      "Iteration 471, loss = 0.00464267\n",
      "Iteration 472, loss = 0.00454971\n",
      "Iteration 473, loss = 0.00454655\n",
      "Iteration 474, loss = 0.00466173\n",
      "Iteration 475, loss = 0.00442388\n",
      "Iteration 476, loss = 0.00446284\n",
      "Iteration 477, loss = 0.00450577\n",
      "Iteration 478, loss = 0.00445855\n",
      "Iteration 479, loss = 0.00440917\n",
      "Iteration 480, loss = 0.00433974\n",
      "Iteration 481, loss = 0.00442107\n",
      "Iteration 482, loss = 0.00431033\n",
      "Iteration 483, loss = 0.00434235\n",
      "Iteration 484, loss = 0.00430040\n",
      "Iteration 485, loss = 0.00426252\n",
      "Iteration 486, loss = 0.00426448\n",
      "Iteration 487, loss = 0.00424773\n",
      "Iteration 488, loss = 0.00431107\n",
      "Iteration 489, loss = 0.00420485\n",
      "Iteration 490, loss = 0.00420364\n",
      "Iteration 491, loss = 0.00418435\n",
      "Iteration 492, loss = 0.00424562\n",
      "Iteration 493, loss = 0.00423513\n",
      "Iteration 494, loss = 0.00417733\n",
      "Iteration 495, loss = 0.00408412\n",
      "Iteration 496, loss = 0.00410095\n",
      "Iteration 497, loss = 0.00411384\n",
      "Iteration 498, loss = 0.00403692\n",
      "Iteration 499, loss = 0.00412598\n",
      "Iteration 500, loss = 0.00401984\n",
      "Iteration 501, loss = 0.00399364\n",
      "Iteration 502, loss = 0.00401755\n",
      "Iteration 503, loss = 0.00392405\n",
      "Iteration 504, loss = 0.00400864\n",
      "Iteration 505, loss = 0.00397095\n",
      "Iteration 506, loss = 0.00393140\n",
      "Iteration 507, loss = 0.00395626\n",
      "Iteration 508, loss = 0.00380222\n",
      "Iteration 509, loss = 0.00391134\n",
      "Iteration 510, loss = 0.00388208\n",
      "Iteration 511, loss = 0.00385520\n",
      "Iteration 512, loss = 0.00380525\n",
      "Iteration 513, loss = 0.00378176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 514, loss = 0.00380607\n",
      "Iteration 515, loss = 0.00381649\n",
      "Iteration 516, loss = 0.00375527\n",
      "Iteration 517, loss = 0.00373968\n",
      "Iteration 518, loss = 0.00369581\n",
      "Iteration 519, loss = 0.00372301\n",
      "Iteration 520, loss = 0.00364756\n",
      "Iteration 521, loss = 0.00363268\n",
      "Iteration 522, loss = 0.00370053\n",
      "Iteration 523, loss = 0.00373211\n",
      "Iteration 524, loss = 0.00362364\n",
      "Iteration 525, loss = 0.00363630\n",
      "Iteration 526, loss = 0.00385450\n",
      "Iteration 527, loss = 0.00361337\n",
      "Iteration 528, loss = 0.00366230\n",
      "Iteration 529, loss = 0.00372207\n",
      "Iteration 530, loss = 0.00355041\n",
      "Iteration 531, loss = 0.00361125\n",
      "Iteration 532, loss = 0.00354025\n",
      "Iteration 533, loss = 0.00348087\n",
      "Iteration 534, loss = 0.00347708\n",
      "Iteration 535, loss = 0.00343112\n",
      "Iteration 536, loss = 0.00350678\n",
      "Iteration 537, loss = 0.00345515\n",
      "Iteration 538, loss = 0.00342482\n",
      "Iteration 539, loss = 0.00342447\n",
      "Iteration 540, loss = 0.00328797\n",
      "Iteration 541, loss = 0.00345794\n",
      "Iteration 542, loss = 0.00339423\n",
      "Iteration 543, loss = 0.00351409\n",
      "Iteration 544, loss = 0.00328956\n",
      "Iteration 545, loss = 0.00331327\n",
      "Iteration 546, loss = 0.00328488\n",
      "Iteration 547, loss = 0.00323090\n",
      "Iteration 548, loss = 0.00350346\n",
      "Iteration 549, loss = 0.00338894\n",
      "Iteration 550, loss = 0.00324389\n",
      "Iteration 551, loss = 0.00318077\n",
      "Iteration 552, loss = 0.00331981\n",
      "Iteration 553, loss = 0.00326156\n",
      "Iteration 554, loss = 0.00322053\n",
      "Iteration 555, loss = 0.00328873\n",
      "Iteration 556, loss = 0.00314115\n",
      "Iteration 557, loss = 0.00313902\n",
      "Iteration 558, loss = 0.00315424\n",
      "Iteration 559, loss = 0.00315720\n",
      "Iteration 560, loss = 0.00315343\n",
      "Iteration 561, loss = 0.00330925\n",
      "Iteration 562, loss = 0.00315328\n",
      "Iteration 563, loss = 0.00316877\n",
      "Iteration 564, loss = 0.00304468\n",
      "Iteration 565, loss = 0.00303768\n",
      "Iteration 566, loss = 0.00299100\n",
      "Iteration 567, loss = 0.00301610\n",
      "Iteration 568, loss = 0.00306332\n",
      "Iteration 569, loss = 0.00300777\n",
      "Iteration 570, loss = 0.00303311\n",
      "Iteration 571, loss = 0.00301636\n",
      "Iteration 572, loss = 0.00293999\n",
      "Iteration 573, loss = 0.00290684\n",
      "Iteration 574, loss = 0.00289915\n",
      "Iteration 575, loss = 0.00293667\n",
      "Iteration 576, loss = 0.00293991\n",
      "Iteration 577, loss = 0.00294802\n",
      "Iteration 578, loss = 0.00291575\n",
      "Iteration 579, loss = 0.00285963\n",
      "Iteration 580, loss = 0.00290828\n",
      "Iteration 581, loss = 0.00290515\n",
      "Iteration 582, loss = 0.00284760\n",
      "Iteration 583, loss = 0.00285796\n",
      "Iteration 584, loss = 0.00304239\n",
      "Iteration 585, loss = 0.00291068\n",
      "Iteration 586, loss = 0.00276164\n",
      "Iteration 587, loss = 0.00277268\n",
      "Iteration 588, loss = 0.00284789\n",
      "Iteration 589, loss = 0.00271940\n",
      "Iteration 590, loss = 0.00282666\n",
      "Iteration 591, loss = 0.00269062\n",
      "Iteration 592, loss = 0.00276980\n",
      "Iteration 593, loss = 0.00264183\n",
      "Iteration 594, loss = 0.00271497\n",
      "Iteration 595, loss = 0.00279265\n",
      "Iteration 596, loss = 0.00264001\n",
      "Iteration 597, loss = 0.00267359\n",
      "Iteration 598, loss = 0.00269253\n",
      "Iteration 599, loss = 0.00264753\n",
      "Iteration 600, loss = 0.00262883\n",
      "Iteration 601, loss = 0.00263905\n",
      "Iteration 602, loss = 0.00257988\n",
      "Iteration 603, loss = 0.00268071\n",
      "Iteration 604, loss = 0.00265665\n",
      "Iteration 605, loss = 0.00263867\n",
      "Iteration 606, loss = 0.00260980\n",
      "Iteration 607, loss = 0.00250481\n",
      "Iteration 608, loss = 0.00266066\n",
      "Iteration 609, loss = 0.00257188\n",
      "Iteration 610, loss = 0.00254347\n",
      "Iteration 611, loss = 0.00263438\n",
      "Iteration 612, loss = 0.00245842\n",
      "Iteration 613, loss = 0.00246022\n",
      "Iteration 614, loss = 0.00246907\n",
      "Iteration 615, loss = 0.00251992\n",
      "Iteration 616, loss = 0.00251149\n",
      "Iteration 617, loss = 0.00251921\n",
      "Iteration 618, loss = 0.00241201\n",
      "Iteration 619, loss = 0.00243718\n",
      "Iteration 620, loss = 0.00251237\n",
      "Iteration 621, loss = 0.00242418\n",
      "Iteration 622, loss = 0.00249376\n",
      "Iteration 623, loss = 0.00235973\n",
      "Iteration 624, loss = 0.00247814\n",
      "Iteration 625, loss = 0.00243359\n",
      "Iteration 626, loss = 0.00247663\n",
      "Iteration 627, loss = 0.00240549\n",
      "Iteration 628, loss = 0.00238946\n",
      "Iteration 629, loss = 0.00231291\n",
      "Iteration 630, loss = 0.00238916\n",
      "Iteration 631, loss = 0.00233889\n",
      "Iteration 632, loss = 0.00232230\n",
      "Iteration 633, loss = 0.00230776\n",
      "Iteration 634, loss = 0.00228954\n",
      "Iteration 635, loss = 0.00224622\n",
      "Iteration 636, loss = 0.00225761\n",
      "Iteration 637, loss = 0.00226249\n",
      "Iteration 638, loss = 0.00220817\n",
      "Iteration 639, loss = 0.00223781\n",
      "Iteration 640, loss = 0.00221564\n",
      "Iteration 641, loss = 0.00229023\n",
      "Iteration 642, loss = 0.00223411\n",
      "Iteration 643, loss = 0.00226931\n",
      "Iteration 644, loss = 0.00226729\n",
      "Iteration 645, loss = 0.00221277\n",
      "Iteration 646, loss = 0.00221485\n",
      "Iteration 647, loss = 0.00217300\n",
      "Iteration 648, loss = 0.00217807\n",
      "Iteration 649, loss = 0.00222658\n",
      "Iteration 650, loss = 0.00212943\n",
      "Iteration 651, loss = 0.00217306\n",
      "Iteration 652, loss = 0.00218965\n",
      "Iteration 653, loss = 0.00222276\n",
      "Iteration 654, loss = 0.00208719\n",
      "Iteration 655, loss = 0.00210173\n",
      "Iteration 656, loss = 0.00207885\n",
      "Iteration 657, loss = 0.00208074\n",
      "Iteration 658, loss = 0.00209831\n",
      "Iteration 659, loss = 0.00208086\n",
      "Iteration 660, loss = 0.00204580\n",
      "Iteration 661, loss = 0.00205708\n",
      "Iteration 662, loss = 0.00203558\n",
      "Iteration 663, loss = 0.00207603\n",
      "Iteration 664, loss = 0.00203180\n",
      "Iteration 665, loss = 0.00199384\n",
      "Iteration 666, loss = 0.00201067\n",
      "Iteration 667, loss = 0.00196777\n",
      "Iteration 668, loss = 0.00197457\n",
      "Iteration 669, loss = 0.00198157\n",
      "Iteration 670, loss = 0.00196826\n",
      "Iteration 671, loss = 0.00201055\n",
      "Iteration 672, loss = 0.00203238\n",
      "Iteration 673, loss = 0.00195866\n",
      "Iteration 674, loss = 0.00205326\n",
      "Iteration 675, loss = 0.00191657\n",
      "Iteration 676, loss = 0.00195706\n",
      "Iteration 677, loss = 0.00192830\n",
      "Iteration 678, loss = 0.00190627\n",
      "Iteration 679, loss = 0.00195149\n",
      "Iteration 680, loss = 0.00190574\n",
      "Iteration 681, loss = 0.00188885\n",
      "Iteration 682, loss = 0.00185848\n",
      "Iteration 683, loss = 0.00190917\n",
      "Iteration 684, loss = 0.00185292\n",
      "Iteration 685, loss = 0.00184183\n",
      "Iteration 686, loss = 0.00183329\n",
      "Iteration 687, loss = 0.00198094\n",
      "Iteration 688, loss = 0.00183195\n",
      "Iteration 689, loss = 0.00188976\n",
      "Iteration 690, loss = 0.00179998\n",
      "Iteration 691, loss = 0.00181743\n",
      "Iteration 692, loss = 0.00184388\n",
      "Iteration 693, loss = 0.00186404\n",
      "Iteration 694, loss = 0.00181283\n",
      "Iteration 695, loss = 0.00177029\n",
      "Iteration 696, loss = 0.00192524\n",
      "Iteration 697, loss = 0.00185969\n",
      "Iteration 698, loss = 0.00184443\n",
      "Iteration 699, loss = 0.00173014\n",
      "Iteration 700, loss = 0.00179597\n",
      "Iteration 701, loss = 0.00180289\n",
      "Iteration 702, loss = 0.00174415\n",
      "Iteration 703, loss = 0.00173939\n",
      "Iteration 704, loss = 0.00174568\n",
      "Iteration 705, loss = 0.00173764\n",
      "Iteration 706, loss = 0.00182755\n",
      "Iteration 707, loss = 0.00177807\n",
      "Iteration 708, loss = 0.00165114\n",
      "Iteration 709, loss = 0.00173329\n",
      "Iteration 710, loss = 0.00172200\n",
      "Iteration 711, loss = 0.00166433\n",
      "Iteration 712, loss = 0.00168442\n",
      "Iteration 713, loss = 0.00165360\n",
      "Iteration 714, loss = 0.00169733\n",
      "Iteration 715, loss = 0.00161693\n",
      "Iteration 716, loss = 0.00182494\n",
      "Iteration 717, loss = 0.00160997\n",
      "Iteration 718, loss = 0.00178859\n",
      "Iteration 719, loss = 0.00152712\n",
      "Iteration 720, loss = 0.00176832\n",
      "Iteration 721, loss = 0.00160419\n",
      "Iteration 722, loss = 0.00174172\n",
      "Iteration 723, loss = 0.00166399\n",
      "Iteration 724, loss = 0.00159788\n",
      "Iteration 725, loss = 0.00160755\n",
      "Iteration 726, loss = 0.00160854\n",
      "Iteration 727, loss = 0.00155534\n",
      "Iteration 728, loss = 0.00157527\n",
      "Iteration 729, loss = 0.00158288\n",
      "Iteration 730, loss = 0.00156982\n",
      "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(20, 20), max_iter=1500, tol=1e-05,\n",
       "              verbose=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rede_neural_credit = MLPClassifier(max_iter=1500, verbose=True, tol=0.0000100,\n",
    "                                   solver = 'adam', activation = 'relu',\n",
    "                                   hidden_layer_sizes = (20,20))\n",
    "rede_neural_credit.fit(X_credit_treinamento, y_credit_treinamento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "previsoes = rede_neural_credit.predict(X_credit_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.998"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_credit_teste, previsoes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.classifier import ConfusionMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.998"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = ConfusionMatrix(rede_neural_credit)\n",
    "cm.fit(X_credit_treinamento, y_credit_treinamento)\n",
    "cm.score(X_credit_teste, y_credit_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       436\n",
      "           1       0.98      1.00      0.99        64\n",
      "\n",
      "    accuracy                           1.00       500\n",
      "   macro avg       0.99      1.00      1.00       500\n",
      "weighted avg       1.00      1.00      1.00       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_credit_teste, previsoes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base do Censo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(caminho + 'census.pkl', 'rb') as f:\n",
    "    X_census_treinamento, y_census_treinamento, X_census_teste, y_census_teste = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.56741320\n",
      "Iteration 2, loss = 0.53766246\n",
      "Iteration 3, loss = 0.52303530\n",
      "Iteration 4, loss = 0.51240661\n",
      "Iteration 5, loss = 0.50703202\n",
      "Iteration 6, loss = 0.50437931\n",
      "Iteration 7, loss = 0.50296713\n",
      "Iteration 8, loss = 0.50292707\n",
      "Iteration 9, loss = 0.50214271\n",
      "Iteration 10, loss = 0.50136056\n",
      "Iteration 11, loss = 0.50068203\n",
      "Iteration 12, loss = 0.50053984\n",
      "Iteration 13, loss = 0.49953201\n",
      "Iteration 14, loss = 0.50028947\n",
      "Iteration 15, loss = 0.49854783\n",
      "Iteration 16, loss = 0.49855681\n",
      "Iteration 17, loss = 0.49823777\n",
      "Iteration 18, loss = 0.49797682\n",
      "Iteration 19, loss = 0.49696373\n",
      "Iteration 20, loss = 0.49708780\n",
      "Iteration 21, loss = 0.49767252\n",
      "Iteration 22, loss = 0.49711159\n",
      "Iteration 23, loss = 0.49648518\n",
      "Iteration 24, loss = 0.49670918\n",
      "Iteration 25, loss = 0.49672405\n",
      "Iteration 26, loss = 0.49608927\n",
      "Iteration 27, loss = 0.49661984\n",
      "Iteration 28, loss = 0.49545057\n",
      "Iteration 29, loss = 0.49497600\n",
      "Iteration 30, loss = 0.49497116\n",
      "Iteration 31, loss = 0.49564194\n",
      "Iteration 32, loss = 0.49499707\n",
      "Iteration 33, loss = 0.49486533\n",
      "Iteration 34, loss = 0.49487278\n",
      "Iteration 35, loss = 0.49557032\n",
      "Iteration 36, loss = 0.49395771\n",
      "Iteration 37, loss = 0.49407453\n",
      "Iteration 38, loss = 0.49367113\n",
      "Iteration 39, loss = 0.49445070\n",
      "Iteration 40, loss = 0.49416394\n",
      "Iteration 41, loss = 0.49406918\n",
      "Iteration 42, loss = 0.49516842\n",
      "Iteration 43, loss = 0.49398385\n",
      "Iteration 44, loss = 0.49421697\n",
      "Iteration 45, loss = 0.49354492\n",
      "Iteration 46, loss = 0.49463268\n",
      "Iteration 47, loss = 0.49366846\n",
      "Iteration 48, loss = 0.49396450\n",
      "Iteration 49, loss = 0.49311495\n",
      "Iteration 50, loss = 0.49373439\n",
      "Iteration 51, loss = 0.49419145\n",
      "Iteration 52, loss = 0.49385318\n",
      "Iteration 53, loss = 0.49327955\n",
      "Iteration 54, loss = 0.49304635\n",
      "Iteration 55, loss = 0.49366720\n",
      "Iteration 56, loss = 0.49353832\n",
      "Iteration 57, loss = 0.49484993\n",
      "Iteration 58, loss = 0.49506042\n",
      "Iteration 59, loss = 0.49373152\n",
      "Iteration 60, loss = 0.49389445\n",
      "Iteration 61, loss = 0.49346051\n",
      "Iteration 62, loss = 0.49328031\n",
      "Iteration 63, loss = 0.49384803\n",
      "Iteration 64, loss = 0.49358459\n",
      "Iteration 65, loss = 0.49337916\n",
      "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(55, 55), max_iter=1000, tol=1e-05,\n",
       "              verbose=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rede_neural_census = MLPClassifier(verbose=True, max_iter = 1000, tol=0.000010,\n",
    "                                  hidden_layer_sizes = (55,55))\n",
    "rede_neural_census.fit(X_census_treinamento, y_census_treinamento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([' <=50K', ' <=50K', ' <=50K', ..., ' <=50K', ' <=50K', ' <=50K'],\n",
       "      dtype='<U6')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previsoes = rede_neural_census.predict(X_census_teste)\n",
    "previsoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([' <=50K', ' <=50K', ' <=50K', ..., ' <=50K', ' <=50K', ' <=50K'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_census_teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.795496417604913"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_census_teste, previsoes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.classifier import ConfusionMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.795496417604913"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = ConfusionMatrix(rede_neural_census)\n",
    "cm.fit(X_census_treinamento, y_census_treinamento)\n",
    "cm.score(X_census_teste, y_census_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <=50K       0.80      0.98      0.88      3693\n",
      "        >50K       0.76      0.24      0.36      1192\n",
      "\n",
      "    accuracy                           0.80      4885\n",
      "   macro avg       0.78      0.61      0.62      4885\n",
      "weighted avg       0.79      0.80      0.75      4885\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_census_teste, previsoes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Salvar o Classificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(rede_neural_census, open(caminho + 'rede_neural_finalizado.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abrir o Classificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "rede_neural = pickle.load(open(caminho + 'rede_neural_finalizado.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
