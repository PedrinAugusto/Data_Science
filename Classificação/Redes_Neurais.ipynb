{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes Neurais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# caminho da pasta onde está o arquivo \n",
    "caminho = 'C:/Users/User/OneDrive/Capacitação/Portifolio_GitHub/Data_Science/data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base de Crédito"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(caminho + 'credit.pkl', 'rb') as f:\n",
    "    X_credit_treinamento, y_credit_treinamento, X_credit_teste, y_credit_teste = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.64279669\n",
      "Iteration 2, loss = 0.59482052\n",
      "Iteration 3, loss = 0.55357315\n",
      "Iteration 4, loss = 0.51662755\n",
      "Iteration 5, loss = 0.48533853\n",
      "Iteration 6, loss = 0.45795381\n",
      "Iteration 7, loss = 0.43413306\n",
      "Iteration 8, loss = 0.41250314\n",
      "Iteration 9, loss = 0.39288284\n",
      "Iteration 10, loss = 0.37436079\n",
      "Iteration 11, loss = 0.35714331\n",
      "Iteration 12, loss = 0.34012028\n",
      "Iteration 13, loss = 0.32410929\n",
      "Iteration 14, loss = 0.30857334\n",
      "Iteration 15, loss = 0.29400349\n",
      "Iteration 16, loss = 0.27974153\n",
      "Iteration 17, loss = 0.26625396\n",
      "Iteration 18, loss = 0.25338753\n",
      "Iteration 19, loss = 0.24127549\n",
      "Iteration 20, loss = 0.22997674\n",
      "Iteration 21, loss = 0.21928812\n",
      "Iteration 22, loss = 0.20931470\n",
      "Iteration 23, loss = 0.19995312\n",
      "Iteration 24, loss = 0.19116194\n",
      "Iteration 25, loss = 0.18276325\n",
      "Iteration 26, loss = 0.17509355\n",
      "Iteration 27, loss = 0.16742091\n",
      "Iteration 28, loss = 0.16042455\n",
      "Iteration 29, loss = 0.15402236\n",
      "Iteration 30, loss = 0.14768738\n",
      "Iteration 31, loss = 0.14162958\n",
      "Iteration 32, loss = 0.13613100\n",
      "Iteration 33, loss = 0.13076778\n",
      "Iteration 34, loss = 0.12586323\n",
      "Iteration 35, loss = 0.12126842\n",
      "Iteration 36, loss = 0.11673053\n",
      "Iteration 37, loss = 0.11258368\n",
      "Iteration 38, loss = 0.10884129\n",
      "Iteration 39, loss = 0.10516640\n",
      "Iteration 40, loss = 0.10173714\n",
      "Iteration 41, loss = 0.09852037\n",
      "Iteration 42, loss = 0.09559677\n",
      "Iteration 43, loss = 0.09250890\n",
      "Iteration 44, loss = 0.08985494\n",
      "Iteration 45, loss = 0.08710947\n",
      "Iteration 46, loss = 0.08457312\n",
      "Iteration 47, loss = 0.08221677\n",
      "Iteration 48, loss = 0.07995895\n",
      "Iteration 49, loss = 0.07797152\n",
      "Iteration 50, loss = 0.07601902\n",
      "Iteration 51, loss = 0.07420105\n",
      "Iteration 52, loss = 0.07253902\n",
      "Iteration 53, loss = 0.07083150\n",
      "Iteration 54, loss = 0.06937044\n",
      "Iteration 55, loss = 0.06806080\n",
      "Iteration 56, loss = 0.06649940\n",
      "Iteration 57, loss = 0.06503862\n",
      "Iteration 58, loss = 0.06375865\n",
      "Iteration 59, loss = 0.06256716\n",
      "Iteration 60, loss = 0.06138866\n",
      "Iteration 61, loss = 0.06008669\n",
      "Iteration 62, loss = 0.05899842\n",
      "Iteration 63, loss = 0.05777343\n",
      "Iteration 64, loss = 0.05667712\n",
      "Iteration 65, loss = 0.05567001\n",
      "Iteration 66, loss = 0.05456892\n",
      "Iteration 67, loss = 0.05377752\n",
      "Iteration 68, loss = 0.05272842\n",
      "Iteration 69, loss = 0.05184059\n",
      "Iteration 70, loss = 0.05110674\n",
      "Iteration 71, loss = 0.05019795\n",
      "Iteration 72, loss = 0.04949924\n",
      "Iteration 73, loss = 0.04869980\n",
      "Iteration 74, loss = 0.04825761\n",
      "Iteration 75, loss = 0.04723055\n",
      "Iteration 76, loss = 0.04672123\n",
      "Iteration 77, loss = 0.04590968\n",
      "Iteration 78, loss = 0.04524585\n",
      "Iteration 79, loss = 0.04456984\n",
      "Iteration 80, loss = 0.04385208\n",
      "Iteration 81, loss = 0.04331045\n",
      "Iteration 82, loss = 0.04261759\n",
      "Iteration 83, loss = 0.04206661\n",
      "Iteration 84, loss = 0.04164888\n",
      "Iteration 85, loss = 0.04095825\n",
      "Iteration 86, loss = 0.04042969\n",
      "Iteration 87, loss = 0.03986635\n",
      "Iteration 88, loss = 0.03929505\n",
      "Iteration 89, loss = 0.03877248\n",
      "Iteration 90, loss = 0.03835355\n",
      "Iteration 91, loss = 0.03777555\n",
      "Iteration 92, loss = 0.03737877\n",
      "Iteration 93, loss = 0.03690628\n",
      "Iteration 94, loss = 0.03636287\n",
      "Iteration 95, loss = 0.03614040\n",
      "Iteration 96, loss = 0.03564006\n",
      "Iteration 97, loss = 0.03514606\n",
      "Iteration 98, loss = 0.03477519\n",
      "Iteration 99, loss = 0.03427877\n",
      "Iteration 100, loss = 0.03391699\n",
      "Iteration 101, loss = 0.03351557\n",
      "Iteration 102, loss = 0.03301255\n",
      "Iteration 103, loss = 0.03270939\n",
      "Iteration 104, loss = 0.03233086\n",
      "Iteration 105, loss = 0.03189098\n",
      "Iteration 106, loss = 0.03158672\n",
      "Iteration 107, loss = 0.03127229\n",
      "Iteration 108, loss = 0.03086334\n",
      "Iteration 109, loss = 0.03059044\n",
      "Iteration 110, loss = 0.03022076\n",
      "Iteration 111, loss = 0.02976275\n",
      "Iteration 112, loss = 0.02958257\n",
      "Iteration 113, loss = 0.02936032\n",
      "Iteration 114, loss = 0.02906963\n",
      "Iteration 115, loss = 0.02869384\n",
      "Iteration 116, loss = 0.02836363\n",
      "Iteration 117, loss = 0.02804221\n",
      "Iteration 118, loss = 0.02771819\n",
      "Iteration 119, loss = 0.02747322\n",
      "Iteration 120, loss = 0.02722955\n",
      "Iteration 121, loss = 0.02711378\n",
      "Iteration 122, loss = 0.02659246\n",
      "Iteration 123, loss = 0.02653070\n",
      "Iteration 124, loss = 0.02617809\n",
      "Iteration 125, loss = 0.02584745\n",
      "Iteration 126, loss = 0.02567678\n",
      "Iteration 127, loss = 0.02539214\n",
      "Iteration 128, loss = 0.02526236\n",
      "Iteration 129, loss = 0.02488093\n",
      "Iteration 130, loss = 0.02467823\n",
      "Iteration 131, loss = 0.02455562\n",
      "Iteration 132, loss = 0.02418848\n",
      "Iteration 133, loss = 0.02411603\n",
      "Iteration 134, loss = 0.02383906\n",
      "Iteration 135, loss = 0.02369383\n",
      "Iteration 136, loss = 0.02346718\n",
      "Iteration 137, loss = 0.02318900\n",
      "Iteration 138, loss = 0.02299537\n",
      "Iteration 139, loss = 0.02285376\n",
      "Iteration 140, loss = 0.02266165\n",
      "Iteration 141, loss = 0.02243927\n",
      "Iteration 142, loss = 0.02221936\n",
      "Iteration 143, loss = 0.02205221\n",
      "Iteration 144, loss = 0.02193834\n",
      "Iteration 145, loss = 0.02170965\n",
      "Iteration 146, loss = 0.02152977\n",
      "Iteration 147, loss = 0.02136719\n",
      "Iteration 148, loss = 0.02118797\n",
      "Iteration 149, loss = 0.02101928\n",
      "Iteration 150, loss = 0.02089972\n",
      "Iteration 151, loss = 0.02067482\n",
      "Iteration 152, loss = 0.02048227\n",
      "Iteration 153, loss = 0.02033831\n",
      "Iteration 154, loss = 0.02030388\n",
      "Iteration 155, loss = 0.01998570\n",
      "Iteration 156, loss = 0.01985349\n",
      "Iteration 157, loss = 0.01966339\n",
      "Iteration 158, loss = 0.01954328\n",
      "Iteration 159, loss = 0.01934403\n",
      "Iteration 160, loss = 0.01920175\n",
      "Iteration 161, loss = 0.01909694\n",
      "Iteration 162, loss = 0.01911190\n",
      "Iteration 163, loss = 0.01889272\n",
      "Iteration 164, loss = 0.01854412\n",
      "Iteration 165, loss = 0.01851988\n",
      "Iteration 166, loss = 0.01839960\n",
      "Iteration 167, loss = 0.01818484\n",
      "Iteration 168, loss = 0.01801246\n",
      "Iteration 169, loss = 0.01794284\n",
      "Iteration 170, loss = 0.01779046\n",
      "Iteration 171, loss = 0.01769955\n",
      "Iteration 172, loss = 0.01754462\n",
      "Iteration 173, loss = 0.01735845\n",
      "Iteration 174, loss = 0.01741175\n",
      "Iteration 175, loss = 0.01724768\n",
      "Iteration 176, loss = 0.01717989\n",
      "Iteration 177, loss = 0.01701584\n",
      "Iteration 178, loss = 0.01679632\n",
      "Iteration 179, loss = 0.01682809\n",
      "Iteration 180, loss = 0.01668760\n",
      "Iteration 181, loss = 0.01652301\n",
      "Iteration 182, loss = 0.01637823\n",
      "Iteration 183, loss = 0.01624628\n",
      "Iteration 184, loss = 0.01618212\n",
      "Iteration 185, loss = 0.01595621\n",
      "Iteration 186, loss = 0.01592773\n",
      "Iteration 187, loss = 0.01574280\n",
      "Iteration 188, loss = 0.01564683\n",
      "Iteration 189, loss = 0.01560297\n",
      "Iteration 190, loss = 0.01542088\n",
      "Iteration 191, loss = 0.01540009\n",
      "Iteration 192, loss = 0.01523870\n",
      "Iteration 193, loss = 0.01517527\n",
      "Iteration 194, loss = 0.01512913\n",
      "Iteration 195, loss = 0.01498031\n",
      "Iteration 196, loss = 0.01490280\n",
      "Iteration 197, loss = 0.01481238\n",
      "Iteration 198, loss = 0.01469135\n",
      "Iteration 199, loss = 0.01459220\n",
      "Iteration 200, loss = 0.01450481\n",
      "Iteration 201, loss = 0.01461050\n",
      "Iteration 202, loss = 0.01434672\n",
      "Iteration 203, loss = 0.01426627\n",
      "Iteration 204, loss = 0.01424319\n",
      "Iteration 205, loss = 0.01415991\n",
      "Iteration 206, loss = 0.01408911\n",
      "Iteration 207, loss = 0.01396486\n",
      "Iteration 208, loss = 0.01387793\n",
      "Iteration 209, loss = 0.01364636\n",
      "Iteration 210, loss = 0.01375328\n",
      "Iteration 211, loss = 0.01362600\n",
      "Iteration 212, loss = 0.01350790\n",
      "Iteration 213, loss = 0.01342716\n",
      "Iteration 214, loss = 0.01336967\n",
      "Iteration 215, loss = 0.01327600\n",
      "Iteration 216, loss = 0.01323203\n",
      "Iteration 217, loss = 0.01304854\n",
      "Iteration 218, loss = 0.01297515\n",
      "Iteration 219, loss = 0.01297518\n",
      "Iteration 220, loss = 0.01292231\n",
      "Iteration 221, loss = 0.01277362\n",
      "Iteration 222, loss = 0.01273819\n",
      "Iteration 223, loss = 0.01275206\n",
      "Iteration 224, loss = 0.01257082\n",
      "Iteration 225, loss = 0.01246273\n",
      "Iteration 226, loss = 0.01248812\n",
      "Iteration 227, loss = 0.01240523\n",
      "Iteration 228, loss = 0.01235652\n",
      "Iteration 229, loss = 0.01220299\n",
      "Iteration 230, loss = 0.01214227\n",
      "Iteration 231, loss = 0.01215749\n",
      "Iteration 232, loss = 0.01218499\n",
      "Iteration 233, loss = 0.01186161\n",
      "Iteration 234, loss = 0.01193876\n",
      "Iteration 235, loss = 0.01185987\n",
      "Iteration 236, loss = 0.01166953\n",
      "Iteration 237, loss = 0.01164013\n",
      "Iteration 238, loss = 0.01155939\n",
      "Iteration 239, loss = 0.01151685\n",
      "Iteration 240, loss = 0.01141821\n",
      "Iteration 241, loss = 0.01143631\n",
      "Iteration 242, loss = 0.01143400\n",
      "Iteration 243, loss = 0.01140964\n",
      "Iteration 244, loss = 0.01123785\n",
      "Iteration 245, loss = 0.01115777\n",
      "Iteration 246, loss = 0.01110473\n",
      "Iteration 247, loss = 0.01106149\n",
      "Iteration 248, loss = 0.01097736\n",
      "Iteration 249, loss = 0.01094149\n",
      "Iteration 250, loss = 0.01083379\n",
      "Iteration 251, loss = 0.01080359\n",
      "Iteration 252, loss = 0.01076773\n",
      "Iteration 253, loss = 0.01065208\n",
      "Iteration 254, loss = 0.01059735\n",
      "Iteration 255, loss = 0.01074754\n",
      "Iteration 256, loss = 0.01075927\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 257, loss = 0.01050349\n",
      "Iteration 258, loss = 0.01035112\n",
      "Iteration 259, loss = 0.01037971\n",
      "Iteration 260, loss = 0.01041414\n",
      "Iteration 261, loss = 0.01044564\n",
      "Iteration 262, loss = 0.01018791\n",
      "Iteration 263, loss = 0.01011562\n",
      "Iteration 264, loss = 0.01009698\n",
      "Iteration 265, loss = 0.01000659\n",
      "Iteration 266, loss = 0.00998304\n",
      "Iteration 267, loss = 0.00991183\n",
      "Iteration 268, loss = 0.00988461\n",
      "Iteration 269, loss = 0.00981516\n",
      "Iteration 270, loss = 0.00974080\n",
      "Iteration 271, loss = 0.00976550\n",
      "Iteration 272, loss = 0.00977315\n",
      "Iteration 273, loss = 0.00969563\n",
      "Iteration 274, loss = 0.00968701\n",
      "Iteration 275, loss = 0.00951065\n",
      "Iteration 276, loss = 0.00947146\n",
      "Iteration 277, loss = 0.00945135\n",
      "Iteration 278, loss = 0.00938295\n",
      "Iteration 279, loss = 0.00936448\n",
      "Iteration 280, loss = 0.00933137\n",
      "Iteration 281, loss = 0.00922819\n",
      "Iteration 282, loss = 0.00925143\n",
      "Iteration 283, loss = 0.00920602\n",
      "Iteration 284, loss = 0.00914741\n",
      "Iteration 285, loss = 0.00903967\n",
      "Iteration 286, loss = 0.00900359\n",
      "Iteration 287, loss = 0.00900554\n",
      "Iteration 288, loss = 0.00893520\n",
      "Iteration 289, loss = 0.00925564\n",
      "Iteration 290, loss = 0.00884079\n",
      "Iteration 291, loss = 0.00882975\n",
      "Iteration 292, loss = 0.00889070\n",
      "Iteration 293, loss = 0.00882976\n",
      "Iteration 294, loss = 0.00873079\n",
      "Iteration 295, loss = 0.00864165\n",
      "Iteration 296, loss = 0.00867507\n",
      "Iteration 297, loss = 0.00864882\n",
      "Iteration 298, loss = 0.00859917\n",
      "Iteration 299, loss = 0.00854048\n",
      "Iteration 300, loss = 0.00852039\n",
      "Iteration 301, loss = 0.00835495\n",
      "Iteration 302, loss = 0.00840515\n",
      "Iteration 303, loss = 0.00838048\n",
      "Iteration 304, loss = 0.00830229\n",
      "Iteration 305, loss = 0.00829119\n",
      "Iteration 306, loss = 0.00813533\n",
      "Iteration 307, loss = 0.00824331\n",
      "Iteration 308, loss = 0.00811074\n",
      "Iteration 309, loss = 0.00814514\n",
      "Iteration 310, loss = 0.00805519\n",
      "Iteration 311, loss = 0.00800016\n",
      "Iteration 312, loss = 0.00805836\n",
      "Iteration 313, loss = 0.00790846\n",
      "Iteration 314, loss = 0.00791227\n",
      "Iteration 315, loss = 0.00801329\n",
      "Iteration 316, loss = 0.00782027\n",
      "Iteration 317, loss = 0.00784730\n",
      "Iteration 318, loss = 0.00789039\n",
      "Iteration 319, loss = 0.00777077\n",
      "Iteration 320, loss = 0.00781042\n",
      "Iteration 321, loss = 0.00774108\n",
      "Iteration 322, loss = 0.00768884\n",
      "Iteration 323, loss = 0.00766700\n",
      "Iteration 324, loss = 0.00764557\n",
      "Iteration 325, loss = 0.00751068\n",
      "Iteration 326, loss = 0.00760788\n",
      "Iteration 327, loss = 0.00769300\n",
      "Iteration 328, loss = 0.00743708\n",
      "Iteration 329, loss = 0.00746242\n",
      "Iteration 330, loss = 0.00737967\n",
      "Iteration 331, loss = 0.00729768\n",
      "Iteration 332, loss = 0.00731413\n",
      "Iteration 333, loss = 0.00730423\n",
      "Iteration 334, loss = 0.00719219\n",
      "Iteration 335, loss = 0.00716846\n",
      "Iteration 336, loss = 0.00723120\n",
      "Iteration 337, loss = 0.00715549\n",
      "Iteration 338, loss = 0.00708314\n",
      "Iteration 339, loss = 0.00715126\n",
      "Iteration 340, loss = 0.00702943\n",
      "Iteration 341, loss = 0.00703260\n",
      "Iteration 342, loss = 0.00694213\n",
      "Iteration 343, loss = 0.00697780\n",
      "Iteration 344, loss = 0.00695193\n",
      "Iteration 345, loss = 0.00686692\n",
      "Iteration 346, loss = 0.00684066\n",
      "Iteration 347, loss = 0.00676486\n",
      "Iteration 348, loss = 0.00680583\n",
      "Iteration 349, loss = 0.00673008\n",
      "Iteration 350, loss = 0.00680407\n",
      "Iteration 351, loss = 0.00668684\n",
      "Iteration 352, loss = 0.00672231\n",
      "Iteration 353, loss = 0.00654538\n",
      "Iteration 354, loss = 0.00659302\n",
      "Iteration 355, loss = 0.00654624\n",
      "Iteration 356, loss = 0.00649428\n",
      "Iteration 357, loss = 0.00648056\n",
      "Iteration 358, loss = 0.00645035\n",
      "Iteration 359, loss = 0.00643404\n",
      "Iteration 360, loss = 0.00643088\n",
      "Iteration 361, loss = 0.00634251\n",
      "Iteration 362, loss = 0.00651454\n",
      "Iteration 363, loss = 0.00632329\n",
      "Iteration 364, loss = 0.00626882\n",
      "Iteration 365, loss = 0.00629276\n",
      "Iteration 366, loss = 0.00623205\n",
      "Iteration 367, loss = 0.00619707\n",
      "Iteration 368, loss = 0.00622632\n",
      "Iteration 369, loss = 0.00616275\n",
      "Iteration 370, loss = 0.00616826\n",
      "Iteration 371, loss = 0.00617546\n",
      "Iteration 372, loss = 0.00604904\n",
      "Iteration 373, loss = 0.00603847\n",
      "Iteration 374, loss = 0.00599495\n",
      "Iteration 375, loss = 0.00594195\n",
      "Iteration 376, loss = 0.00592772\n",
      "Iteration 377, loss = 0.00596569\n",
      "Iteration 378, loss = 0.00597390\n",
      "Iteration 379, loss = 0.00589949\n",
      "Iteration 380, loss = 0.00597289\n",
      "Iteration 381, loss = 0.00587085\n",
      "Iteration 382, loss = 0.00578643\n",
      "Iteration 383, loss = 0.00585833\n",
      "Iteration 384, loss = 0.00576568\n",
      "Iteration 385, loss = 0.00571386\n",
      "Iteration 386, loss = 0.00571799\n",
      "Iteration 387, loss = 0.00564517\n",
      "Iteration 388, loss = 0.00564335\n",
      "Iteration 389, loss = 0.00564242\n",
      "Iteration 390, loss = 0.00561689\n",
      "Iteration 391, loss = 0.00560319\n",
      "Iteration 392, loss = 0.00558322\n",
      "Iteration 393, loss = 0.00556760\n",
      "Iteration 394, loss = 0.00558200\n",
      "Iteration 395, loss = 0.00546833\n",
      "Iteration 396, loss = 0.00549909\n",
      "Iteration 397, loss = 0.00546653\n",
      "Iteration 398, loss = 0.00540501\n",
      "Iteration 399, loss = 0.00547690\n",
      "Iteration 400, loss = 0.00535095\n",
      "Iteration 401, loss = 0.00536942\n",
      "Iteration 402, loss = 0.00532677\n",
      "Iteration 403, loss = 0.00544238\n",
      "Iteration 404, loss = 0.00526153\n",
      "Iteration 405, loss = 0.00528482\n",
      "Iteration 406, loss = 0.00529942\n",
      "Iteration 407, loss = 0.00526078\n",
      "Iteration 408, loss = 0.00516491\n",
      "Iteration 409, loss = 0.00515613\n",
      "Iteration 410, loss = 0.00514470\n",
      "Iteration 411, loss = 0.00509378\n",
      "Iteration 412, loss = 0.00517063\n",
      "Iteration 413, loss = 0.00507981\n",
      "Iteration 414, loss = 0.00513653\n",
      "Iteration 415, loss = 0.00514824\n",
      "Iteration 416, loss = 0.00498724\n",
      "Iteration 417, loss = 0.00504005\n",
      "Iteration 418, loss = 0.00495088\n",
      "Iteration 419, loss = 0.00497799\n",
      "Iteration 420, loss = 0.00496471\n",
      "Iteration 421, loss = 0.00488228\n",
      "Iteration 422, loss = 0.00487875\n",
      "Iteration 423, loss = 0.00497370\n",
      "Iteration 424, loss = 0.00491721\n",
      "Iteration 425, loss = 0.00487953\n",
      "Iteration 426, loss = 0.00483405\n",
      "Iteration 427, loss = 0.00480853\n",
      "Iteration 428, loss = 0.00482189\n",
      "Iteration 429, loss = 0.00483094\n",
      "Iteration 430, loss = 0.00477771\n",
      "Iteration 431, loss = 0.00487822\n",
      "Iteration 432, loss = 0.00476637\n",
      "Iteration 433, loss = 0.00476500\n",
      "Iteration 434, loss = 0.00464049\n",
      "Iteration 435, loss = 0.00467527\n",
      "Iteration 436, loss = 0.00460092\n",
      "Iteration 437, loss = 0.00459031\n",
      "Iteration 438, loss = 0.00457394\n",
      "Iteration 439, loss = 0.00465220\n",
      "Iteration 440, loss = 0.00452034\n",
      "Iteration 441, loss = 0.00449492\n",
      "Iteration 442, loss = 0.00447460\n",
      "Iteration 443, loss = 0.00447068\n",
      "Iteration 444, loss = 0.00446828\n",
      "Iteration 445, loss = 0.00454442\n",
      "Iteration 446, loss = 0.00449077\n",
      "Iteration 447, loss = 0.00458035\n",
      "Iteration 448, loss = 0.00441977\n",
      "Iteration 449, loss = 0.00449444\n",
      "Iteration 450, loss = 0.00435570\n",
      "Iteration 451, loss = 0.00432005\n",
      "Iteration 452, loss = 0.00437446\n",
      "Iteration 453, loss = 0.00437107\n",
      "Iteration 454, loss = 0.00436900\n",
      "Iteration 455, loss = 0.00437800\n",
      "Iteration 456, loss = 0.00433211\n",
      "Iteration 457, loss = 0.00427865\n",
      "Iteration 458, loss = 0.00436860\n",
      "Iteration 459, loss = 0.00430056\n",
      "Iteration 460, loss = 0.00418477\n",
      "Iteration 461, loss = 0.00417167\n",
      "Iteration 462, loss = 0.00412981\n",
      "Iteration 463, loss = 0.00410946\n",
      "Iteration 464, loss = 0.00416240\n",
      "Iteration 465, loss = 0.00402480\n",
      "Iteration 466, loss = 0.00406167\n",
      "Iteration 467, loss = 0.00420688\n",
      "Iteration 468, loss = 0.00412437\n",
      "Iteration 469, loss = 0.00409014\n",
      "Iteration 470, loss = 0.00410479\n",
      "Iteration 471, loss = 0.00399103\n",
      "Iteration 472, loss = 0.00401829\n",
      "Iteration 473, loss = 0.00398085\n",
      "Iteration 474, loss = 0.00408705\n",
      "Iteration 475, loss = 0.00399599\n",
      "Iteration 476, loss = 0.00404425\n",
      "Iteration 477, loss = 0.00392897\n",
      "Iteration 478, loss = 0.00392383\n",
      "Iteration 479, loss = 0.00398576\n",
      "Iteration 480, loss = 0.00393075\n",
      "Iteration 481, loss = 0.00392982\n",
      "Iteration 482, loss = 0.00382258\n",
      "Iteration 483, loss = 0.00380909\n",
      "Iteration 484, loss = 0.00394230\n",
      "Iteration 485, loss = 0.00404618\n",
      "Iteration 486, loss = 0.00381341\n",
      "Iteration 487, loss = 0.00374675\n",
      "Iteration 488, loss = 0.00378689\n",
      "Iteration 489, loss = 0.00371228\n",
      "Iteration 490, loss = 0.00380876\n",
      "Iteration 491, loss = 0.00370308\n",
      "Iteration 492, loss = 0.00372303\n",
      "Iteration 493, loss = 0.00366974\n",
      "Iteration 494, loss = 0.00367992\n",
      "Iteration 495, loss = 0.00360850\n",
      "Iteration 496, loss = 0.00360516\n",
      "Iteration 497, loss = 0.00362630\n",
      "Iteration 498, loss = 0.00365309\n",
      "Iteration 499, loss = 0.00356435\n",
      "Iteration 500, loss = 0.00361729\n",
      "Iteration 501, loss = 0.00356773\n",
      "Iteration 502, loss = 0.00354555\n",
      "Iteration 503, loss = 0.00357073\n",
      "Iteration 504, loss = 0.00363777\n",
      "Iteration 505, loss = 0.00353613\n",
      "Iteration 506, loss = 0.00347771\n",
      "Iteration 507, loss = 0.00345208\n",
      "Iteration 508, loss = 0.00348259\n",
      "Iteration 509, loss = 0.00340944\n",
      "Iteration 510, loss = 0.00354386\n",
      "Iteration 511, loss = 0.00343217\n",
      "Iteration 512, loss = 0.00339635\n",
      "Iteration 513, loss = 0.00338535\n",
      "Iteration 514, loss = 0.00340765\n",
      "Iteration 515, loss = 0.00339715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 516, loss = 0.00337738\n",
      "Iteration 517, loss = 0.00342827\n",
      "Iteration 518, loss = 0.00335365\n",
      "Iteration 519, loss = 0.00342895\n",
      "Iteration 520, loss = 0.00342675\n",
      "Iteration 521, loss = 0.00332518\n",
      "Iteration 522, loss = 0.00335409\n",
      "Iteration 523, loss = 0.00329104\n",
      "Iteration 524, loss = 0.00346021\n",
      "Iteration 525, loss = 0.00337460\n",
      "Iteration 526, loss = 0.00317972\n",
      "Iteration 527, loss = 0.00333690\n",
      "Iteration 528, loss = 0.00321451\n",
      "Iteration 529, loss = 0.00322895\n",
      "Iteration 530, loss = 0.00319643\n",
      "Iteration 531, loss = 0.00316727\n",
      "Iteration 532, loss = 0.00324071\n",
      "Iteration 533, loss = 0.00315684\n",
      "Iteration 534, loss = 0.00320413\n",
      "Iteration 535, loss = 0.00316938\n",
      "Iteration 536, loss = 0.00307173\n",
      "Iteration 537, loss = 0.00318537\n",
      "Iteration 538, loss = 0.00302010\n",
      "Iteration 539, loss = 0.00315142\n",
      "Iteration 540, loss = 0.00311497\n",
      "Iteration 541, loss = 0.00303015\n",
      "Iteration 542, loss = 0.00308116\n",
      "Iteration 543, loss = 0.00302394\n",
      "Iteration 544, loss = 0.00302991\n",
      "Iteration 545, loss = 0.00307578\n",
      "Iteration 546, loss = 0.00299465\n",
      "Iteration 547, loss = 0.00296096\n",
      "Iteration 548, loss = 0.00299409\n",
      "Iteration 549, loss = 0.00295117\n",
      "Iteration 550, loss = 0.00293651\n",
      "Iteration 551, loss = 0.00297135\n",
      "Iteration 552, loss = 0.00296122\n",
      "Iteration 553, loss = 0.00292459\n",
      "Iteration 554, loss = 0.00295183\n",
      "Iteration 555, loss = 0.00287485\n",
      "Iteration 556, loss = 0.00287744\n",
      "Iteration 557, loss = 0.00291412\n",
      "Iteration 558, loss = 0.00290363\n",
      "Iteration 559, loss = 0.00284633\n",
      "Iteration 560, loss = 0.00288765\n",
      "Iteration 561, loss = 0.00281472\n",
      "Iteration 562, loss = 0.00288557\n",
      "Iteration 563, loss = 0.00281692\n",
      "Iteration 564, loss = 0.00283602\n",
      "Iteration 565, loss = 0.00279248\n",
      "Iteration 566, loss = 0.00278470\n",
      "Iteration 567, loss = 0.00279842\n",
      "Iteration 568, loss = 0.00290065\n",
      "Iteration 569, loss = 0.00293242\n",
      "Iteration 570, loss = 0.00278071\n",
      "Iteration 571, loss = 0.00278807\n",
      "Iteration 572, loss = 0.00275029\n",
      "Iteration 573, loss = 0.00263755\n",
      "Iteration 574, loss = 0.00271363\n",
      "Iteration 575, loss = 0.00273634\n",
      "Iteration 576, loss = 0.00269178\n",
      "Iteration 577, loss = 0.00268684\n",
      "Iteration 578, loss = 0.00269153\n",
      "Iteration 579, loss = 0.00263767\n",
      "Iteration 580, loss = 0.00267906\n",
      "Iteration 581, loss = 0.00262942\n",
      "Iteration 582, loss = 0.00269769\n",
      "Iteration 583, loss = 0.00264996\n",
      "Iteration 584, loss = 0.00270031\n",
      "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(20, 20), max_iter=1500, tol=1e-05,\n",
       "              verbose=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rede_neural_credit = MLPClassifier(max_iter=1500, verbose=True, tol=0.0000100,\n",
    "                                   solver = 'adam', activation = 'relu',\n",
    "                                   hidden_layer_sizes = (20,20))\n",
    "rede_neural_credit.fit(X_credit_treinamento, y_credit_treinamento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "previsoes = rede_neural_credit.predict(X_credit_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.994"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_credit_teste, previsoes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.classifier import ConfusionMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.994"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdoAAAFHCAYAAAAGHI0yAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAAANwklEQVR4nO3ce5CdBX3G8WfXLAsJuRgUEiAYBRIvAYVGEZqSkVSHgEZJK0gZmyC2UFBKEUoogkA7AUqCU1AJFcTKpUGshMaQoCJYoIUZkQTihZ1GQ9JCgrhK8UiSDXv6BzWOcglTzy+H7H4+M5nJvu/uO89ONvnOOec96Wg2m80AACU62z0AAAYyoQWAQkILAIWEFgAKCS0AFBrS6gv29/en0Wikq6srHR0drb48ALyiNJvN9PX1ZdiwYensfP7j15aHttFopKenp9WXBYBXtAkTJmT48OHPO97y0HZ1dSVJ7j3h/Gx4orfVlwdewl/++FtJVrZ7BgwqmzYlPT2/7t9va3lof/V08YYnevPM40+2+vLAS+ju7m73BBi0XuzlUjdDAUAhoQWAQkILAIWEFgAKCS0AFBJaACgktABQSGgBoJDQAkAhoQWAQkILAIWEFgAKCS0AFBJaACgktABQSGgBoJDQAkAhoQWAQkILAIWEFgAKCS0AFBJaACgktABQSGgBoJDQAkAhoQWAQkILAIWEFgAKCS0AFBJaACgktABQSGgBoJDQAkAhoQWAQkILAIWEFgAKCS0AFBJaACgktABQSGgBoJDQAkAhoQWAQkILAIWEFgAKCS0AFBJaACgktABQSGgBoJDQAkAhoQWAQkILAIWEFgAKCS0AFBJaACgktABQSGgBoJDQAkAhoQWAQkILAIWEFgAKCS0AFBJaACgktABQSGgBoJDQAkAhoQWAQkILAIWEdhCa+P5pmfPUA0mSzq6uvHfBBTn5e0ty8veW5D3zzkpH53M/FrtOmpCzn/5uTnxw0ZZfu0x4fTunw4DVbDYze/b5mTfvunZPocWGvJxPuuuuuzJ//vxs2rQpEydOzNy5c7PzzjtXb6PA6H1e938x7UiSvONjx2Xoa0fnc5Pem47Ozhx/9w15y9HTs3Lhkow75IA8fOPX8rUTz2vzahjYfvCDH+eUUy7Jffc9nEmT9m73HFpsq49oe3t7c/bZZ+eKK67I7bffnnHjxmXevHnbYhstNmSnHXPU9Zfm9tMv3nLsvk9/MV855q+SZjNDdxmVHUeNyDO9TyVJ9jzkgLzmTXvno/ffnI/ef3PeeNS72zUdBrTPfvbLOf749+Xoo/0dG4i2Gtp77rkn++23X8aPH58kOfbYY7N48eI0m83qbbTYe6+6MA9cdVPWP/TIbxzv37w50y76RE5d9Y001j+ZR+/+TpKkr/FMVt74tVx90AezaNZZOfLK8zP2wLe0YzoMaJ/5zFn58IePbPcMimw1tOvWrcuYMWO2fDxmzJj84he/SKPRKB1Ga03+iz9J/+bNWX7tv7zg+TvOnp9LXv2O/Hz1f+fIK89Pktx2ygX5zoJ/TpI8+cMf5ftfXpqJMw7bVpMBBoSthra/v/+Fv7DTfVTbk7fNPip7vH2/nPjgohx32z9myE475sQHF2XcIQdm9L7jkzz3yHb5F2/J2APfnI7OzvzB35yUHXYe9uuLdHTk2b7N7fkGALZTW70ZauzYsVmxYsWWj9evX5+RI0dm6NChpcNorasP+uCW34983R45eeXiXHXAB3LoJ0/OHu98axa+/+Q0+/uz/3Hvy+pv3Z9mf38mzDgsmzdszH9cdm1G7rV73vRH78mXDpvVxu8CYPuz1YelU6ZMyYoVK7J69eokycKFCzNt2rTqXWwj91zy+Tz16GM5acWtOWnFrenf/Gy+efb8JMlXjzsj+0w/NCc99K85bunnc/tpc/PkD3/U5sUA25eO5su4q+nb3/525s+fn76+vuy111655JJLMmrUqBf83I0bN2blypW5432n5pnHn2z1XuAlfKr5SJIH2j0DBpWNG5OVK5NJkyalu7v7eedf1vtop06dmqlTp7Z8HAAMdO5oAoBCQgsAhYQWAAoJLQAUEloAKCS0AFBIaAGgkNACQCGhBYBCQgsAhYQWAAoJLQAUEloAKCS0AFBIaAGgkNACQCGhBYBCQgsAhYQWAAoJLQAUEloAKCS0AFBIaAGgkNACQCGhBYBCQgsAhYQWAAoJLQAUEloAKCS0AFBIaAGgkNACQCGhBYBCQgsAhYQWAAoJLQAUEloAKCS0AFBIaAGgkNACQCGhBYBCQgsAhYQWAAoJLQAUEloAKCS0AFBIaAGgkNACQCGhBYBCQgsAhYQWAAoJLQAUEloAKCS0AFBIaAGgkNACQCGhBYBCQgsAhYQWAAoJLQAUEloAKCS0AFBIaAGgkNACQCGhBYBCQgsAhYQWAAoJLQAUEloAKDSk6sLXjuzN+g0/qbo88AI+lST5vTavgMFmY5KVL3q2LLTLl1+f7u6qqwMvZPTo0en9z0+3ewYMLn1dSSa+6GlPHQNAIaEFgEJCCwCFhBYACgktABQSWgAoJLQAUEhoAaCQ0AJAIaEFgEJCCwCFhBYACgktABQSWgAoJLQAUEhoAaCQ0AJAIaEFgEJCCwCFhBYACgktABQSWgAoJLQAUEhoAaCQ0AJAIaEFgEJCCwCFhBYACgktABQSWgAoJLQAUEhoAaCQ0AJAIaEFgEJCCwCFhBYACgktABQSWgAoJLQAUEhoAaCQ0AJAIaEFgEJCCwCFhBYACgktABQSWgAoJLQAUEhoAaCQ0AJAIaEFgEJCCwCFhBYACgktABQSWgAoJLQAUEhoAaCQ0AJAIaEFgEJCCwCFhBYACgktABQSWgAoJLQAUEhoAaCQ0AJAIaEFgEJCCwCFhBYACg1p9wBeGa6//rZceul16ejoyNChO+byy8/I5MlvbvcsGJAe/v7afHzO9Xnqf57Jqzo7c9Vls3PA/ntlzoU3Z8nXV6SzszP7vmG3XHXZrLz2NSPaPZff0ct6RNtsNjNnzpxcc8011Xtog0ceWZ0zz/yHLFt2RZYvvzGf/ORHMnPmme2eBQPSL3+5Me/543n5648fkQfvujDnnjEjx524IF+44e48sHx1vnvnBXn4nr/LPm/YNZ84d2G759ICWw3tqlWrMmvWrCxdunRb7KENurt3yNVXn5uxY1+TJJk8+c1Zt+6n2bSpr83LYOD5+p0rs/f4XXPEu9+aJJkx/YB8+Qun5C1v3COXXnBMuru7kiST3/b6PPpfP23nVFpkq08d33DDDZk5c2Z23333bbGHNhg/fveMH//cn2+z2czpp386M2Ycmh126GrzMhh4elatz5hdR+aEU6/JipVrM2rk0Pz9+Ufn4Lfvs+VzfvbzRi689NacNPtdbVxKq2w1tOedd16S5L777isfQ3s1Gs9k9uzzs3bt+ixbdkW758CA1Ne3Obd986HcueisHDR579x623dzxIcuy6PL56e7uyurfvxEPvDhyzPlnfvmlI9Oa/dcWsBdxyRJ1qxZl0MO+Uhe9arO3HnngowaNbzdk2BA2n3Mq/PGfcfmoMl7J0nef8SBefbZZn60+ie58+4f5ODD/zazPvT7WTB/djo6Otq8llYQWtLb+1SmTv3zzJz5rixceFF22mnHdk+CAWv6H+6X1WuezAPLVydJ/u3fH0lHR/Kzpxo56k8vz5c+92c542PT2zuSlvL2HnLllV/JmjXrcsstd+WWW+7acvyOOz6XXXYZ1bZdMBCN2W1UFl13ak4+80tp/HJjuruH5Kv/9PF86uJFaTaTORfenDkX3pwkef1er80t153a5sX8roSWnHPOCTnnnBPaPQMGjUMPmZj7v3Hebxz7xle9pW6getmhvfjiiyt3AMCA5DVaACgktABQSGgBoJDQAkAhoQWAQkILAIWEFgAKCS0AFBJaACgktABQSGgBoJDQAkAhoQWAQkILAIWEFgAKCS0AFBJaACgktABQSGgBoJDQAkAhoQWAQkILAIWEFgAKCS0AFBJaACgktABQSGgBoJDQAkAhoQWAQkILAIWEFgAKCS0AFBJaACgktABQSGgBoJDQAkAhoQWAQkILAIWEFgAKCS0AFBJaACgktABQSGgBoJDQAkAhoQWAQkILAIWEFgAKCS0AFBJaACgktABQSGgBoJDQAkAhoQWAQkILAIWEFgAKCS0AFBJaACgktABQSGgBoJDQAkAhoQWAQkILAIWEFgAKCS0AFBJaACgktABQSGgBoJDQAkAhoQWAQkNafcFms5kk2bSp1VcGtma33XbLxr6uds+AQWXT5udS+qv+/baO5oud+X96+umn09PT08pLAsAr3oQJEzJ8+PDnHW95aPv7+9NoNNLV1ZWOjo5WXhoAXnGazWb6+voybNiwdHY+/xXZlocWAPg1N0MBQCGhBYBCQgsAhYQWAAoJLQAUElqSJI1GIxs2bGj3DIABp+X/MxTbj0ajkXnz5mXx4sVpNBpJkhEjRmTatGmZM2dORowY0eaFANs/76MdxE477bTsueeeOfbYYzNmzJgkybp163LTTTelp6cnCxYsaPNCgO2f0A5i06dPz9KlS1/w3JFHHpklS5Zs40UweFx77bUvef7444/fRkuo5qnjQayrqytr167NuHHjfuP4mjVrMmSIHw2o1NPTk2XLluXwww9v9xSK+dd0EDv99NNzzDHHZP/999/y1PETTzyRhx56KHPnzm3zOhjYLrroojz22GM5+OCDM2PGjHbPoZCnjge53t7e3HvvvXn88cfTbDYzduzYTJkyJaNHj273NBjwVq1alRtvvDHnnntuu6dQSGgBoJD30QJAIaEFgEJCCwCFhBYACgktABT6X887qcgLoSaLAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = ConfusionMatrix(rede_neural_credit)\n",
    "cm.fit(X_credit_treinamento, y_credit_treinamento)\n",
    "cm.score(X_credit_teste, y_credit_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       436\n",
      "           1       0.98      0.97      0.98        64\n",
      "\n",
      "    accuracy                           0.99       500\n",
      "   macro avg       0.99      0.98      0.99       500\n",
      "weighted avg       0.99      0.99      0.99       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_credit_teste, previsoes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base do Censo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(caminho + 'census.pkl', 'rb') as f:\n",
    "    X_census_treinamento, y_census_treinamento, X_census_teste, y_census_teste = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.56741320\n",
      "Iteration 2, loss = 0.53766246\n",
      "Iteration 3, loss = 0.52303530\n",
      "Iteration 4, loss = 0.51240661\n",
      "Iteration 5, loss = 0.50703202\n",
      "Iteration 6, loss = 0.50437931\n",
      "Iteration 7, loss = 0.50296713\n",
      "Iteration 8, loss = 0.50292707\n",
      "Iteration 9, loss = 0.50214271\n",
      "Iteration 10, loss = 0.50136056\n",
      "Iteration 11, loss = 0.50068203\n",
      "Iteration 12, loss = 0.50053984\n",
      "Iteration 13, loss = 0.49953201\n",
      "Iteration 14, loss = 0.50028947\n",
      "Iteration 15, loss = 0.49854783\n",
      "Iteration 16, loss = 0.49855681\n",
      "Iteration 17, loss = 0.49823777\n",
      "Iteration 18, loss = 0.49797682\n",
      "Iteration 19, loss = 0.49696373\n",
      "Iteration 20, loss = 0.49708780\n",
      "Iteration 21, loss = 0.49767252\n",
      "Iteration 22, loss = 0.49711159\n",
      "Iteration 23, loss = 0.49648518\n",
      "Iteration 24, loss = 0.49670918\n",
      "Iteration 25, loss = 0.49672405\n",
      "Iteration 26, loss = 0.49608927\n",
      "Iteration 27, loss = 0.49661984\n",
      "Iteration 28, loss = 0.49545057\n",
      "Iteration 29, loss = 0.49497600\n",
      "Iteration 30, loss = 0.49497116\n",
      "Iteration 31, loss = 0.49564194\n",
      "Iteration 32, loss = 0.49499707\n",
      "Iteration 33, loss = 0.49486533\n",
      "Iteration 34, loss = 0.49487278\n",
      "Iteration 35, loss = 0.49557032\n",
      "Iteration 36, loss = 0.49395771\n",
      "Iteration 37, loss = 0.49407453\n",
      "Iteration 38, loss = 0.49367113\n",
      "Iteration 39, loss = 0.49445070\n",
      "Iteration 40, loss = 0.49416394\n",
      "Iteration 41, loss = 0.49406918\n",
      "Iteration 42, loss = 0.49516842\n",
      "Iteration 43, loss = 0.49398385\n",
      "Iteration 44, loss = 0.49421697\n",
      "Iteration 45, loss = 0.49354492\n",
      "Iteration 46, loss = 0.49463268\n",
      "Iteration 47, loss = 0.49366846\n",
      "Iteration 48, loss = 0.49396450\n",
      "Iteration 49, loss = 0.49311495\n",
      "Iteration 50, loss = 0.49373439\n",
      "Iteration 51, loss = 0.49419145\n",
      "Iteration 52, loss = 0.49385318\n",
      "Iteration 53, loss = 0.49327955\n",
      "Iteration 54, loss = 0.49304635\n",
      "Iteration 55, loss = 0.49366720\n",
      "Iteration 56, loss = 0.49353832\n",
      "Iteration 57, loss = 0.49484993\n",
      "Iteration 58, loss = 0.49506042\n",
      "Iteration 59, loss = 0.49373152\n",
      "Iteration 60, loss = 0.49389445\n",
      "Iteration 61, loss = 0.49346051\n",
      "Iteration 62, loss = 0.49328031\n",
      "Iteration 63, loss = 0.49384803\n",
      "Iteration 64, loss = 0.49358459\n",
      "Iteration 65, loss = 0.49337916\n",
      "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(55, 55), max_iter=1000, tol=1e-05,\n",
       "              verbose=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rede_neural_census = MLPClassifier(verbose=True, max_iter = 1000, tol=0.000010,\n",
    "                                  hidden_layer_sizes = (55,55))\n",
    "rede_neural_census.fit(X_census_treinamento, y_census_treinamento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([' <=50K', ' <=50K', ' <=50K', ..., ' <=50K', ' <=50K', ' <=50K'],\n",
       "      dtype='<U6')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previsoes = rede_neural_census.predict(X_census_teste)\n",
    "previsoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([' <=50K', ' <=50K', ' <=50K', ..., ' <=50K', ' <=50K', ' <=50K'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_census_teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.795496417604913"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_census_teste, previsoes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.classifier import ConfusionMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.795496417604913"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = ConfusionMatrix(rede_neural_census)\n",
    "cm.fit(X_census_treinamento, y_census_treinamento)\n",
    "cm.score(X_census_teste, y_census_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <=50K       0.80      0.98      0.88      3693\n",
      "        >50K       0.76      0.24      0.36      1192\n",
      "\n",
      "    accuracy                           0.80      4885\n",
      "   macro avg       0.78      0.61      0.62      4885\n",
      "weighted avg       0.79      0.80      0.75      4885\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_census_teste, previsoes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Salvar o Classificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(rede_neural_census, open(caminho + 'rede_neural_finalizado.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abrir o Classificador"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "rede_neural = pickle.load(open(caminho + 'rede_neural_finalizado.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(caminho + 'census.pkl', 'rb') as f:\n",
    "    X_census_treinamento, y_census_treinamento, X_census_teste, y_census_teste = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
